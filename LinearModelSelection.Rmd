---
title: "Linear model selection"
output: html_notebook
---

```{r}
library(glmnet) # alpha = 0 is ridge, 1 is lasso
library(pls)
library(leaps)
prostate = get(load("prostate2.Rdata"))
head(prostate)
prostate$svi = as.factor(prostate$svi)
summary(prostate)
attach(prostate)
```

# Variable evaluation and comparison

```{r}
plot(prostate)
```


```{r}
plot(prostate$Cscore, prostate$lweight)
```

There are 97 data points, seven predictor variables and one response variable. There seem to be positive correlations between Cscore and lpsa as well as lcavol and lpsa.


# Forward stepwise selection

```{r}
fwd = regsubsets(Cscore~., data = prostate, nvmax = 10, method="forward")
summary(fwd)
```

```{r}

bestforward = function (response, data){
  bestModelList = vector(mode="list")
  p = length(names(data))
  k = 0:(p-1)
  null = lm(response~1)
  for (i in k){ #iterate over all variables
    modellist = vector(mode="list")
    for (j in 2:(p-k)){ #iterate over all Mk+1
      
      provFormulaString = paste(response, "~")
      for (z in 2:j){#for the current Mk+1, create the model string
        if (z == 2){
          provFormulaString = paste(provFormulaString, names(data)[z])
        } else {
          provFormulaString = paste(provFormulaString, "+", names(data)[z])
        }
      }
      itermodel = lm(as.formula(provFormulaString), data = data) #assigns the current Mk+1 this model
      modellist[[j]] = itermodel #add this model to the candidate model list
    }
    
    rSquaredList = vector(mode="list")
    for (h in 1:length(modellist)){
      rSquaredList = summary(modellist[[h]])$r.squared
    }
    bestModelNumber = which.max(rSquaredList)
    bestModel = modellist[bestModelNumber]
    bestModelList[[i]] = bestModel
  }
  
  BICforModels = vector(mode="list")
  for (h in 1:length(bestModelList)){
    BICforModels[[h]] = BIC(bestModelList[[h]])
  }
  bestBICModelNumber = which.max(BICforModels)
  return(bestModelList[[bestBICModelNumber]])
}

bestforward(Cscore, prostate)

```

