---
title: "Linear model selection"
output: html_notebook
---

```{r}
library(glmnet) # alpha = 0 is ridge, 1 is lasso
library(pls)
library(leaps)
prostate = get(load("prostate2.Rdata"))
head(prostate)
prostate$svi = as.factor(prostate$svi)
summary(prostate)
attach(prostate)
```

# Variable evaluation and comparison

```{r}
plot(prostate)
```


```{r}
plot(prostate$Cscore, prostate$lweight)
```

There are 97 data points, seven predictor variables and one response variable. There seem to be positive correlations between Cscore and lpsa as well as lcavol and lpsa.


# Forward stepwise selection

Let us try to select a model using forward stepwise selection.

```{r}
fwd = regsubsets(Cscore~., data = prostate, nvmax = 10, method="forward")
summary(fwd)
```

```{r}
bestforward = function (data){
  response = data[1]
  bestModelList = vector(mode="list")
  p = length(names(data))
  k = 0:(p-2)
  nullmod = lm(as.matrix(data[1])~ 1, data = data)
  predictorData = data
  predictorData[,1] = NULL
  bestModelList[[1]] = nullmod
  constructionData = predictorData
  for (i in k){ #iterate over all variables, starting with null model 0 and to p-1
    # now we need to iterate as many times as there are unused variables to construct the candidate model matrix
    modellist = vector(mode="list")
    for (z in 1:length(names(constructionData))){
      itermodel = update(bestModelList[[i + 1]], paste("~ . +", names(constructionData)[z]))
      modellist[[z]] = itermodel
      remove(itermodel)
    }
    
    # select the best model out of this Mk+1 subset, picking the highest RSquared
    rSquaredList = vector(mode="list")
    for (h in 1:length(modellist)){
      rSquaredList[h] = summary(modellist[[h]])$r.squared
    }
    bestModelNumber = which.max(rSquaredList)
    bestModel = modellist[[bestModelNumber]]
    bestModelList[[i+2]] = bestModel
    #discard the variable used in the model from further use
    #constructionData thus keeps track of which variables have been used up
    toBeDiscarded = names(bestModel$coefficients)[-1]
    constructionDataNamespace = names(constructionData)
    variableInDiscardButNotInConstructionData = toBeDiscarded %in% constructionDataNamespace
    toBeDiscarded = toBeDiscarded[variableInDiscardButNotInConstructionData]
    if (length(toBeDiscarded) > 0){
      constructionData = constructionData[,-match(toBeDiscarded, names(constructionData)), drop=F]
      } 
  }
  #now that we have the best models of all Mk+1 subsets, pick the best best model via minimum BIC
  BICforModels = vector(mode="list")
  for (h in 1:length(bestModelList)){
    BICforModels[h] = BIC(bestModelList[[h]])
  }
  bestBICModelNumber = which.min(BICforModels)
  return(bestModelList[[bestBICModelNumber]])
}

result = bestforward(prostate)
result
```

As can be seen, our self-implemented forward stepwise algorithm and the regsubsets function are in agreement on which variables best describe the data. Unlike the algorithm described, regsubsets select the best model for a given number of variables, but our found model agrees with the two variable model selected by regsubsets.


# Lasso

First we must choose an appropriate $\lambda$ parameter. We can do this using k-fold cross-validation, using the best model determined in the previous section.

```{r}
set.seed(10)
cvError = vector(mode="list")
for (i in 1:10){
  itermodel = lm(Cscore~lpsa + svi1)
}

```


