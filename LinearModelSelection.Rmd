---
title: "Linear model selection"
output: html_notebook
---

```{r}
library(glmnet) # alpha = 0 is ridge, 1 is lasso
library(pls)
library(leaps)
prostate = get(load("prostate2.Rdata"))
head(prostate)
prostate$svi = as.factor(prostate$svi)
summary(prostate)
attach(prostate)
```

# Variable evaluation and comparison

```{r}
plot(prostate)
```


```{r}
plot(prostate$Cscore, prostate$lweight)
```

There are 97 data points, seven predictor variables and one response variable. There seem to be positive correlations between Cscore and lpsa as well as lcavol and lpsa.


# Forward stepwise selection

```{r}
fwd = regsubsets(Cscore~., data = prostate, nvmax = 10, method="forward")
summary(fwd)
```

```{r}

bestforward = function (response, nvmax = 19, data){
  modellist = NULL
  p = length(names(data))
  k = 0:(p-1)
  null = lm(response~1)
  for (i in k){
    modellist = c()
    for (j in 2:(p-k)){ #this only does the first variable though
      iterformula = formula(Cscore~data[,j])
      modellist[j] = lm(Cscore~data[,j]) # how to add extra variables to lm?
      
      provFormulaString = paste(response, "~")
      for (z in 2:j){}
        provFormulaString = paste(provFormulaString, "+", names(prostate)[j])
      }
    }
  }
}

```

