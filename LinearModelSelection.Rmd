---
title: "Linear model selection"
output: html_notebook
---

```{r}
library(glmnet) # alpha = 0 is ridge, 1 is lasso
library(pls)
library(leaps)
prostate = get(load("prostate2.Rdata"))
head(prostate)
prostate$svi = as.factor(prostate$svi)
summary(prostate)
attach(prostate)
```

# Variable evaluation and comparison

```{r}
plot(prostate)
```


```{r}
plot(prostate$Cscore, prostate$lweight)
```

There are 97 data points, seven predictor variables and one response variable. There seem to be positive correlations between Cscore and lpsa as well as lcavol and lpsa.


# Forward stepwise selection

```{r}
fwd = regsubsets(Cscore~., data = prostate, nvmax = 10, method="forward")
summary(fwd)
```

# Lasso model
```{r}
library(glmnet)
x = model.matrix(Cscore~.,prostate)[,-1]
y = prostate$Cscore
train=sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]

grid=10^seq(10,-2,length=100) 
lasso.model=glmnet(x[train,],y[train],alpha=1,lambda=grid, thresh = 1e-12)
plot(lasso.model, label = TRUE)
plot(lasso.model,xvar="lambda",label=TRUE)#behaviour coef plotted against log lambda
plot(lasso.model,xvar="dev",label=TRUE)#behaviour coef plotted against percentage deviance explained

cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlambda=cv.out$lambda.min
lasso.pred=predict(lasso.model,s=bestlambda,newx=x[test,])
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlambda)[1:8,]
lasso.coef

```

