---
title: "Linear model selection"
author: Thomas Vanpoucke, Boris Shilov
output: 
  bookdown::pdf_document2:
    number_sections: true
---

```{r echo=FALSE}
library(glmnet) # alpha = 0 is ridge, 1 is lasso
library(pls)
library(leaps)
set.seed(10)
prostate = get(load("prostate2.Rdata"))
prostate$svi = as.factor(prostate$svi)
attach(prostate)
```
```{r}
summary(prostate)
```


# Variable evaluation and comparison

```{r}
plot(prostate)
```


There are 97 data points, seven predictor variables and one response variable. There seem to be strong positive correlations present in the dataset, such as those between Cscore and lpsa as well as lcavol and lpsa.


# Forward stepwise selection

Let us select a model using forward stepwise selection.

```{r}
#split data into training and test subsets
x = model.matrix(Cscore~.,prostate)[,-1]
y = prostate$Cscore
train=sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]
```


```{r}
forward_stepwise <- function(data){
  data_vars = names(data)
  predictor_list = c()
  models <- paste("Mk", 1:6, sep = "")
  for (j in 1:(length(data_vars)-1)){ # -1 because Cscore is not a predictor
    rsq = 0
    best_predictor = NULL
    best_model = NULL
    name = paste("Mk",j, sep = "")
    
    for (i in 2:length(data_vars)){ #Cscore is 1, the response variable
      if (!is.element(data_vars[i],predictor_list)){
        Mki <- lm(paste(data_vars[1], "~", paste(predictor_list, collapse = "+", "+",paste(data_vars[i]))), data = data)
        new_rsq = summary(Mki)$r.squared
        if (new_rsq > rsq){
          rsq = new_rsq
          best_predictor = data_vars[i]
          assign(name, Mki)
        }
      }
    }
    predictor_list <- c(predictor_list, best_predictor)
  }
  model_list <- lapply(models, function(x)get(x))
  
  BIC_models = c()
  for (h in 1:length(model_list)){
    BIC_models = c(BIC_models,BIC(model_list[[h]]))
  }
  best_model_nr = which.min(BIC_models)
  return(model_list[[best_model_nr]])
}

result = forward_stepwise(prostate)
result
```

Using forward stepwise selection, the best model appears to be one incorporating lpsa and svi1 at terms. The best model as determined by BIC is then of the form:
$$
Cscore = 
$$

# Lasso

```{r}
grid=10^seq(10,-2,length=100) 
lasso.model=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh = 1e-12)
OLS.model=glmnet(x[train,],y[train],alpha=0,lambda=0, thresh = 1e-12)
plot(lasso.model, label = TRUE)
plot(lasso.model,xvar="lambda",label=TRUE)#behaviour coef plotted against log lambda
plot(lasso.model,xvar="dev",label=TRUE)#behaviour coef plotted against percentage deviance explained
```

```{r}
cv.lasso.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.lasso.out)
```
The best value of $\lambda$ will minimize both the mean squared error and the variance.

```{r}
bestlambda.lasso=cv.lasso.out$lambda.min
out=glmnet(x,y,alpha=1,lambda=grid)

lasso.pred = predict(lasso.model,s=bestlambda.lasso,newx=x[test,])
lasso.mean = mean((lasso.pred-y.test)^2)
lasso.coef = predict(out,type="coefficients",s=bestlambda.lasso)[1:8,]

OLS.pred = predict(OLS.model, s=0, newx=x[test,])
OLS.mean = mean((OLS.pred-y.test)^2)
OLS.coef = predict(out, type="coefficients", s=0)[1:8,]

lasso.coef
OLS.coef
```

We can clearly see that Lasso zeroes out a large number of our variables, which is a massive reduction of complexity. Two of the variables agree with our previous best forward selection. Thus Lasso is clearly superior to ordinary least squares in this instance. The resulting Lasso model becomes:
$$
Cscore = `r lasso.coef[1]` + `r lasso.coef[6]` \times svi1 + `r lasso.coef[7]` \times lcp + `r lasso.coef[8]` \times lpsa
$$

# Ridge

```{r}
grid=10^seq(10,-2,length=100) 
ridge.model=glmnet(x[train,],y[train],alpha=1,lambda=grid, thresh = 1e-12)
OLS.model.ridge=glmnet(x[train,],y[train],alpha=1,lambda=0, thresh = 1e-12)
plot(ridge.model, label = TRUE)
plot(ridge.model,xvar="lambda",label=TRUE)#behaviour coef plotted against log lambda
plot(ridge.model,xvar="dev",label=TRUE)#behaviour coef plotted against percentage deviance explained
```

```{r}
cv.ridge.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.ridge.out)
bestlambda.ridge=cv.ridge.out$lambda.min
```
Lowest mean squared error here corresponds to lowest cross validation error. We can conclude that the best $\lambda$ value equals `r bestlambda.ridge`.


```{r}
bestlambda.ridge=cv.ridge.out$lambda.min
out.ridge=glmnet(x,y,alpha=1,lambda=grid)

ridge.pred = predict(ridge.model,s=bestlambda.ridge,newx=x[test,])
ridge.mean = mean((ridge.pred-y.test)^2)
ridge.coef = predict(out.ridge,type="coefficients",s=bestlambda.ridge)[1:8,]

OLS.pred.ridge = predict(OLS.model.ridge, s=0, newx=x[test,])
OLS.mean.ridge = mean((OLS.pred.ridge-y.test)^2)
OLS.coef.ridge = predict(out.ridge, type="coefficients", s=0)[1:8,]

ridge.coef
OLS.coef

ridge.mean
OLS.mean.ridge
```

There is still a subsantial improvement over OLS in Ridge as most coefficients are zeroed out. The resulting Ridge model is thus:

$$
Cscore = `r ridge.coef[1]` + `r ridge.coef[6]` \times svi1 + `r ridge.coef[7]` \times lcp + `r ridge.coef[8]` \times lpsa
$$

